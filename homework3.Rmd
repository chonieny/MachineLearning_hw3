---
title: "Homework 3"
author: Na Yun Cho
output: pdf_document
---
```{r}
library(tidyverse) # data manipulation
library(dplyr)
library(ISLR) # for data
library(janitor) # clean names 
library(AppliedPredictiveModeling) # better plots
library(caret) # modeling
library(corrplot) # correlation plots
library(pROC) # ROC curve
library(MASS) # LDA
```

## (a)
```{r}
# Import data
data(Weekly)
weekly1 <-
  Weekly %>% 
  dplyr::select(-Today, -Year)

# Feature plots

theme1 <- transparentTheme(trans = .4)
trellis.par.set(theme1)

featurePlot(x = weekly1[, 1:6], 
            y = weekly1$Direction,
            scales = list(x = list(relation = "free"), 
                          y = list(relation = "free")),
            plot = "density", pch = "|", 
            auto.key = list(columns = 2))

# Correlation plot 
corrplot(cor(weekly1[,-7]), tl.srt = 45, order = 'hclust', type = 'upper')
```
The feature plots show that the distribution of the response classes (Up or Down) highly overlap for each feature. 
The correlation plot shows that there is not much pairwise correlation between the features. 

## (b)
```{r}
weekly <-
  Weekly %>% 
  dplyr::select(-Today)

#training set
train_df =
  weekly %>%
  filter(Year < 2009) %>% 
  dplyr::select(-Year)

#test set
test_df = anti_join(weekly, train_df)

contrasts(weekly$Direction)

#perform logistic regression
glm_train <- glm(Direction ~ ., 
               data = train_df, 
               family = binomial(link = "logit"))
summary(glm_train)
```
It seems that the Lag1 predictor is statistically significant. 

```{r}
#compute the confusion matrix and overall fraction of correct predictions using the test data 
test.pred.prob <- predict(glm_train, newdata = test_df,
                           type = "response")

test.pred <- rep("Down", length(test.pred.prob))
test.pred[test.pred.prob>0.5] <- "Up"

confusionMatrix(data = as.factor(test.pred),
                reference = test_df$Direction,
                positive = "Up")
```
The confusion matrix indicates that the fraction of correct predictions is 0.4615. 
Because the No Information Rate value (0.5865) is higher than the Accuracy value, the classifier is not very meaningful. 
The large P-value also indicates that the accuracy is not significantly better than the No Information Rate. 
In addition, the Kappa Value that is close to 0 indicates that the probability of observed agreement is the same as the probability of agreement by chance. Thus, it indicates that the classification performance is not good. 
The sensitivity (0.2787) is low while the specificity (0.7209) is relatively high. 
Sensitivity measures the proportion of true positives that are correctly predicted and specificity measures the proportion of true negatives that are correctly predicted. 


## (c)
```{r}
glm_train2 <- glm(Direction ~ Lag1 + Lag2,
                 data = train_df, 
                 family = binomial(link = "logit"))


test.pred.prob <- predict(glm_train2, newdata = test_df,
                           type = "response")
test.pred <- rep("neg", length(test.pred.prob))
test.pred[test.pred.prob>0.5] <- "pos"

roc.glm <- roc(test_df$Direction, test.pred.prob)
plot(roc.glm, legacy.axes = TRUE, print.auc = TRUE)
plot(smooth(roc.glm), col = 4, add = TRUE)
```
The AUC is 0.5558. 

## (d) 
```{r}
# LDA
lda.fit <- lda(Direction~ Lag1 + Lag2,
               data = train_df)
lda.pred <- predict(lda.fit, newdata = test_df)
roc.lda <- roc(test_df$Direction, lda.pred$posterior[,2])
plot(roc.lda, legacy.axes = TRUE, print.auc = TRUE)

# QDA
qda.fit <- qda(Direction ~ Lag1 + Lag2,
              data = train_df)
qda.pred <- predict(qda.fit, newdata = test_df)
roc.qda <- roc(test_df$Direction, qda.pred$posterior[,2])
plot(roc.qda, legacy.axes = TRUE, print.auc = TRUE)
               
auc <- c(roc.lda$auc[1], roc.qda$auc[1])
```
The AUC from LDA is 0.55661 and the AUC from QDA is 0.5288. 

## (e)
```{r}
set.seed(10000)
ctrl <- trainControl(method = "repeatedcv",
                     repeats = 5, 
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)

model.knn <- train(x = train_df[,1:2],
                   y = train_df$Direction,
                   method = "knn",
                   preProcess = c("center", "scale"), 
                   tuneGrid = data.frame(k = seq(1, 499, by = 5)),  
                   trControl = ctrl)

model.knn$bestTune
pred_knn = predict(model.knn, newdata = test_df, type = 'prob')
roc_knn <- roc(test_df$Direction, pred_knn[,2])
plot.roc(roc_knn, legacy.axes = TRUE, print.auc = TRUE)
```
The AUC is 0.531.

Discussion of Results: 
I have fit different types of models using the training data from 1990 to 2008 and plotted the ROC curves using the test data from 2009 to 2010. 
From comparing all the AUC values, I can see that LDA generates the largest AUC among these models, which indicates that it shows the best classification performance. 
However, it is also apparent that the AUC's for these four models are similar and slightly above 0.5, which indicates that it is hard to correctly classify the response 'Direction' with the given predictors. 
In addition, the ROC for logistic regression, LDA, and QDA seem relatively stable since they do not involve tuning parameters. On the other hand, the ROC for the KNN method seems relatively less stable. 